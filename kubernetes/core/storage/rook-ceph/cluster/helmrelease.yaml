---
# yaml-language-server: $schema=https://kubernetes-schemas.devbu.io/helm.toolkit.fluxcd.io/helmrelease_v2beta1.json
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
spec:
  interval: 30m
  timeout: 15m
  chart:
    spec:
      chart: rook-ceph-cluster
      version: v1.12.7
      sourceRef:
        kind: HelmRepository
        name: rook-ceph
        namespace: flux-system
  maxHistory: 2
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    # Installs a debugging toolbox deployment
    toolbox:
      enabled: true
      resources:
        limits:
          memory: "105Mi"
        requests:
          cpu: "15m"
          memory: "105Mi"
    monitoring:
      enabled: false

    # -- Create & use PSP resources. Set this to the same value as the rook-ceph chart.
    pspEnable: false

    # imagePullSecrets option allow to pull docker images from private docker registry. Option will be passed to all service accounts.
    # imagePullSecrets:
    # - name: my-registry-secret

    # All values below are taken from the CephCluster CRD
    # -- Cluster configuration.
    # @default -- See [below](#ceph-cluster-spec)
    cephClusterSpec:

      cephVersion:
      dataDirHostPath: /var/lib/rook
      skipUpgradeChecks: false
      continueUpgradeAfterChecksEvenIfNotHealthy: false
      waitTimeoutForHealthyOSDInMinutes: 10

      mon:
        count: 3
        allowMultiplePerNode: false

      mgr:
        count: 1
        allowMultiplePerNode: false
        modules:
          - name: pg_autoscaler
            enabled: true
      dashboard:
        enabled: true
        port: 8443
        ssl: false
      crashCollector:
        disable: false
      cleanupPolicy:
        confirmation: ""

        sanitizeDisks:

          method: quick

          dataSource: zero
          # iteration overwrite N times instead of the default (1)
          # takes an integer value
          iteration: 1
        # allowUninstallWithVolumes defines how the uninstall should be performed
        # If set to true, cephCluster deletion does not wait for the PVs to be deleted.
        allowUninstallWithVolumes: false

      # priority classes to apply to ceph resources
      priorityClassNames:
        mon: system-node-critical
        osd: system-node-critical
        mgr: system-cluster-critical

      storage:
        useAllNodes: false
        useAllDevices: false
        config:
          osdsPerDevice: "1"
        nodes: #talosctl ls /dev/disk/by-id/ -n 10.0.10.84 --endpoints 10.0.10.84 --talosconfig=./clusterconfig/talosconfig
          - name: master-microserver-81
            devices:
              - name: /dev/disk/by-id/nvme-SAMSUNG_MZVKW512HMJP-000L7_S35BNX0K302697
          - name: master-microserver-82
            devices:
              - name: /dev/disk/by-id/nvme-SAMSUNG_MZVKW512HMJP-000L7_S35BNX0K114238
          - name: master-microserver-83
            devices:
              - name: /dev/disk/by-id/nvme-SAMSUNG_MZVKW512HMJP-000L7_S35BNX0K206525
          - name: worker-microserver-84
            devices:
              - name: /dev/disk/by-id/nvme-SAMSUNG_MZVKW512HMJP-000L7_S35BNX0K611110
          - name: worker-microserver-85
            devices:
              - name: /dev/disk/by-id/nvme-SAMSUNG_MZVKW512HMJP-000L7_S35BNX0K205562

        # deviceFilter:
        # config:
        #   crushRoot: "custom-root" # specify a non-default root label for the CRUSH map
        #   metadataDevice: "md0" # specify a non-rotational storage so ceph-volume will use it as block db device of bluestore.
        #   databaseSizeMB: "1024" # uncomment if the disks are smaller than 100 GB
        #   osdsPerDevice: "1" # this value can be overridden at the node or device level
        #   encryptedDevice: "true" # the default value for this option is "false"
        # # Individual nodes and their config can be specified as well, but 'useAllNodes' above must be set to false. Then, only the named
        # # nodes below will be used as storage resources. Each node's 'name' field should match their 'kubernetes.io/hostname' label.
        # nodes:
        #   - name: "172.17.4.201"
        #     devices: # specific devices to use for storage can be specified for each node
        #       - name: "sdb"
        #       - name: "nvme01" # multiple osds can be created on high performance devices
        #         config:
        #           osdsPerDevice: "5"
        #       - name: "/dev/disk/by-id/ata-ST4000DM004-XXXX" # devices can be specified using full udev paths
        #     config: # configuration can be specified at the node level which overrides the cluster level config
        #   - name: "172.17.4.301"
        #     deviceFilter: "^sd."

      # The section for configuring management of daemon disruptions during upgrade or fencing.
      disruptionManagement:
        # If true, the operator will create and manage PodDisruptionBudgets for OSD, Mon, RGW, and MDS daemons. OSD PDBs are managed dynamically
        # via the strategy outlined in the [design](https://github.com/rook/rook/blob/master/design/ceph/ceph-managed-disruptionbudgets.md). The operator will
        # block eviction of OSDs by default and unblock them safely when drains are detected.
        managePodBudgets: true
        # A duration in minutes that determines how long an entire failureDomain like `region/zone/host` will be held in `noout` (in addition to the
        # default DOWN/OUT interval) when it is draining. This is only relevant when  `managePodBudgets` is `true`. The default value is `30` minutes.
        osdMaintenanceTimeout: 30
        # A duration in minutes that the operator will wait for the placement groups to become healthy (active+clean) after a drain was completed and OSDs came back up.
        # Operator will continue with the next drain if the timeout exceeds. It only works if `managePodBudgets` is `true`.
        # No values or 0 means that the operator will wait until the placement groups are healthy before unblocking the next drain.
        pgHealthCheckTimeout: 0

      # Configure the healthcheck and liveness probes for ceph pods.
      # Valid values for daemons are 'mon', 'osd', 'status'
      healthCheck:
        daemonHealth:
          mon:
            disabled: false
            interval: 45s
          osd:
            disabled: false
            interval: 60s
          status:
            disabled: false
            interval: 60s
        # Change pod liveness probe, it works for all mon, mgr, and osd pods.
        livenessProbe:
          mon:
            disabled: false
          mgr:
            disabled: false
          osd:
            disabled: false

    ingress:
      # -- Enable an ingress for the ceph-dashboard
      dashboard:
        {}
        # annotations:
        #   external-dns.alpha.kubernetes.io/hostname: dashboard.example.com
        #   nginx.ingress.kubernetes.io/rewrite-target: /ceph-dashboard/$2
        # If the dashboard has ssl: true the following will make sure the NGINX Ingress controller can expose the dashboard correctly
        #   nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
        #   nginx.ingress.kubernetes.io/server-snippet: |
        #     proxy_ssl_verify off;
        # host:
        #   name: dashboard.example.com
        #   path: "/ceph-dashboard(/|$)(.*)"
        # tls:
        # - hosts:
        #     - dashboard.example.com
        #   secretName: testsecret-tls
        ## Note: Only one of ingress class annotation or the `ingressClassName:` can be used at a time
        ## to set the ingress class
        # ingressClassName: nginx

    # -- A list of CephBlockPool configurations to deploy
    # @default -- See [below](#ceph-block-pools)
    cephBlockPools:
      - name: ceph-blockpool
        # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Block-Storage/ceph-block-pool-crd.md#spec for available configuration
        spec:
          failureDomain: host
          replicated:
            size: 3
          # Enables collecting RBD per-image IO statistics by enabling dynamic OSD performance counters. Defaults to false.
          # For reference: https://docs.ceph.com/docs/master/mgr/prometheus/#rbd-io-statistics
          # enableRBDStats: true
        storageClass:
          enabled: true
          name: ceph-block
          isDefault: true
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          volumeBindingMode: "Immediate"
          mountOptions: []
          # see https://kubernetes.io/docs/concepts/storage/storage-classes/#allowed-topologies
          allowedTopologies: []
          #        - matchLabelExpressions:
          #            - key: rook-ceph-role
          #              values:
          #                - storage-node
          # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Block-Storage-RBD/block-storage.md#provision-storage for available configuration
          parameters:
            # (optional) mapOptions is a comma-separated list of map options.
            # For krbd options refer
            # https://docs.ceph.com/docs/master/man/8/rbd/#kernel-rbd-krbd-options
            # For nbd options refer
            # https://docs.ceph.com/docs/master/man/8/rbd-nbd/#options
            # mapOptions: lock_on_read,queue_depth=1024

            # (optional) unmapOptions is a comma-separated list of unmap options.
            # For krbd options refer
            # https://docs.ceph.com/docs/master/man/8/rbd/#kernel-rbd-krbd-options
            # For nbd options refer
            # https://docs.ceph.com/docs/master/man/8/rbd-nbd/#options
            # unmapOptions: force

            # RBD image format. Defaults to "2".
            imageFormat: "2"

            # RBD image features, equivalent to OR'd bitfield value: 63
            # Available for imageFormat: "2". Older releases of CSI RBD
            # support only the `layering` feature. The Linux kernel (KRBD) supports the
            # full feature complement as of 5.4
            imageFeatures: layering

            # These secrets contain Ceph admin credentials.
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
            # Specify the filesystem type of the volume. If not specified, csi-provisioner
            # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
            # in hyperconverged settings where the volume is mounted on the same node as the osds.
            csi.storage.k8s.io/fstype: ext4

    # -- A list of CephFileSystem configurations to deploy
    # @default -- See [below](#ceph-file-systems)
    cephFileSystems: null
    cephObjectStores: null